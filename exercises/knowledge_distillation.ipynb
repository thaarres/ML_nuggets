{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0976f888",
   "metadata": {},
   "source": [
    "# Knowledge distillation\n",
    "Based on this awesome paper [https://arxiv.org/abs/1503.02531](Distilling the Knowledge in a Neural Network, by Geoffrey Hinton, Oriol Vinyals, Jeff Dean). Made in a rysh, escuse any brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1) #Before: (N, 28, 28) After: (N, 28, 28, 1)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = x_test[0]\n",
    "sample_label = y_test[0]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(sample_image, cmap=\"gray\")\n",
    "plt.title(f\"Label: {sample_label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Label:\", sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will follow the original input and flatten it to use a DNN:\n",
    "# Flatten the MNIST inputs\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create early stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitor validation loss\n",
    "    patience=10,          # Stop if no improvement after 10 epochs\n",
    "    restore_best_weights=True  # Restore model weights from best epoch\n",
    ")\n",
    "#Create teacher\n",
    "def create_teacher():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(784,)),                 # Flattened MNIST input\n",
    "        layers.Dense(1200, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1200, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10)                            # Output logits\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train teacher\n",
    "teacher = create_teacher()\n",
    "teacher.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "teacher.fit(x_train, y_train, epochs=50, batch_size=64, validation_split=0.1,callbacks=[early_stopping])\n",
    "teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429838ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hinton-style student (1 hidden layer, 300 units)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create Hinton-style student (1 hidden layer, 300 units)\n",
    "def create_student():\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(784,)),          # Flattened input\n",
    "        layers.Dense(300, activation='relu'),\n",
    "        layers.Dense(10)                     # Output logits\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "student = create_student()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9ed18",
   "metadata": {},
   "source": [
    "### Knowledge Distillation with KL Divergence\n",
    "\n",
    "In distillation, we train a **student model** to mimic the **softened output** of a **teacher model**.  \n",
    "We use **KL Divergence** to measure how much the student’s predicted distribution diverges from the teacher’s.\n",
    "\n",
    "$$\n",
    "\\text{KL}(P || Q) = \\sum_i P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P \\): teacher's softmax output (soft labels)\n",
    "- \\( Q \\): student’s softmax output\n",
    "\n",
    "> KL divergence encourages the student to match the **class probabilities**, not just the hard label.\n",
    "\n",
    "![KL Divergence Intuition](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Kullback%E2%80%93Leibler_distributions_example_1.svg/2560px-Kullback%E2%80%93Leibler_distributions_example_1.svg.png)\n",
    "\n",
    "See [here](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723c383",
   "metadata": {},
   "source": [
    "## Softmax Temperature in Knowledge Distillation\n",
    "\n",
    "This plot shows how the temperature parameter \\(T\\) affects the softmax output distribution in knowledge distillation:\n",
    "\n",
    "![Softmax Temperature Scaling](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7xj72SjtNHvCMQlV.jpeg)\n",
    "\n",
    "- **T = 1**: Standard softmax with sharp, confident predictions.\n",
    "- **T > 1**: Softens the probability distribution, revealing more information about less likely classes.\n",
    "- **T → ∞**: Produces a uniform distribution over classes.\n",
    "\n",
    "Higher temperature values help the student model learn better from the teacher's softened outputs by capturing relative similarities between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b653a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model class for knowledge distillation\n",
    "# This wraps a student and teacher model and defines training logic\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super().__init__()  # Initialize base keras.Model\n",
    "        self.student = student  # Student model to be trained\n",
    "        self.teacher = teacher  # Pre-trained teacher model\n",
    "\n",
    "    def compile(self, optimizer, metrics,\n",
    "                student_loss_fn, distillation_loss_fn,\n",
    "                alpha=0.1, temperature=10):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data  # Unpack the data (features and labels)\n",
    "\n",
    "        # Get teacher predictions (frozen, inference mode)\n",
    "        teacher_logits = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get student predictions\n",
    "            student_logits = self.student(x, training=True)\n",
    "\n",
    "            # Hard label loss (e.g. true labels)\n",
    "            student_loss = self.student_loss_fn(y, student_logits)\n",
    "\n",
    "            # Soft label loss (between softened predictions)\n",
    "            distill_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_logits / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_logits / self.temperature, axis=1),\n",
    "            )\n",
    "\n",
    "            # Combine losses\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distill_loss\n",
    "\n",
    "        # Compute gradients and apply them\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        # Update and return metrics\n",
    "        self.compiled_metrics.update_state(y, student_logits)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\n",
    "            \"student_loss\": student_loss,\n",
    "            \"distill_loss\": distill_loss,\n",
    "            \"total_loss\": loss,\n",
    "        })\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    " \n",
    "        return self.student(inputs, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student using knowledge distillation with Hinton's original parameters\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "\n",
    "# Compile the distiller\n",
    "distiller.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = distiller.fit(x_train, y_train, epochs=50, batch_size=64, validation_split=0.1,callbacks=[early_stopping])\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4214941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['student_loss'], label='Student Loss')\n",
    "plt.plot(history.history['distill_loss'], label='Distillation Loss')\n",
    "plt.plot(history.history['total_loss'], label='Combined Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38442606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After distillation training\n",
    "student.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Now this will work:\n",
    "student.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1328fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to comnpare with a student trained from skratch:\n",
    "student_scratch = create_student()\n",
    "student_scratch.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train from scratch\n",
    "student_scratch.fit(x_train, y_train, epochs=50, batch_size=64, validation_split=0.1,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mplhep\n",
    "# %pip install mplhep if neccessary\n",
    "# Apply a style (e.g. CMS or ATLAS)\n",
    "mplhep.style.use(\"CMS\")\n",
    "\n",
    "# Binarize the test labels (one-hot for ROC)\n",
    "y_test_bin = label_binarize(y_test, classes=range(10))\n",
    "\n",
    "# Select the digit class to analyze (class 8)\n",
    "target_class = 8\n",
    "\n",
    "# Get probabilities\n",
    "teacher_probs = tf.nn.softmax(teacher(x_test), axis=1).numpy()\n",
    "student_probs = tf.nn.softmax(student(x_test), axis=1).numpy()\n",
    "student_scratch_probs = tf.nn.softmax(student_scratch(x_test), axis=1).numpy()\n",
    "\n",
    "# Compute ROC curves and AUCs\n",
    "fpr_t, tpr_t, _ = roc_curve(y_test_bin[:, target_class], teacher_probs[:, target_class])\n",
    "auc_t = auc(fpr_t, tpr_t)\n",
    "\n",
    "fpr_d, tpr_d, _ = roc_curve(y_test_bin[:, target_class], student_probs[:, target_class])\n",
    "auc_d = auc(fpr_d, tpr_d)\n",
    "\n",
    "fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, target_class], student_scratch_probs[:, target_class])\n",
    "auc_s = auc(fpr_s, tpr_s)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.semilogx(fpr_t, tpr_t, label=f\"Teacher (AUC = {auc_t:.4f})\", linewidth=2)\n",
    "plt.semilogx(fpr_d, tpr_d, label=f\"Distilled Student (AUC = {auc_d:.4f})\", linestyle='--', linewidth=2)\n",
    "plt.semilogx(fpr_s, tpr_s, label=f\"Student from Scratch (AUC = {auc_s:.4f})\", linestyle=':', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logits just to get a feeling where they differ\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get logits from student model (shape: [num_samples, 10])\n",
    "logits = student(x_test).numpy()\n",
    "\n",
    "# Plot only first two dimensions of logits\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(logits[:, 0], logits[:, 1], c=y_test, cmap='tab10', s=10, alpha=0.7)\n",
    "plt.xlabel(\"Logit dimension 0\")\n",
    "plt.ylabel(\"Logit dimension 1\")\n",
    "plt.colorbar(scatter, ticks=range(10), label='True Label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot logits just to get a feeling where they differ\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get logits from student model (shape: [num_samples, 10])\n",
    "logits = teacher(x_test).numpy()\n",
    "\n",
    "# Plot only first two dimensions of logits\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(logits[:, 0], logits[:, 1], c=y_test, cmap='tab10', s=10, alpha=0.7)\n",
    "plt.xlabel(\"Logit dimension 0\")\n",
    "plt.ylabel(\"Logit dimension 1\")\n",
    "plt.colorbar(scatter, ticks=range(10), label='True Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baba3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axol1tl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
