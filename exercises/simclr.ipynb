{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dd2aa09",
   "metadata": {},
   "source": [
    "# SimCLR with MNIST\n",
    "A little ML nugget to reproduce the [SimCLR paper](https://arxiv.org/abs/2002.05709). We'll be using MNIST again here.\n",
    "\n",
    "![simclr](https://amitness.com/posts/images/simclr-general-architecture.png)\n",
    "\n",
    "Sim CLR (ka A Simple Framework for Contrastive Learning of Visual Representations) uses **self-supervised learning** to learn useful representations without any labels, using only augmented views of the input image.\n",
    "\n",
    "- **Contrastive Learning Objective**:\n",
    "  - Bring **positive pairs** (two augmentations of the same image) closer in the embedding space.\n",
    "  - Push **negative pairs** (augmentations from different images)  apart.\n",
    "\n",
    "- **Ingredients**:\n",
    "  - **Data Augmentation**: Each image is randomly augmented **twice** (crop, flip, color jitter, etc.).\n",
    "  - **Encoder Network**: A CNN (e.g., ResNet or ConvNet) maps images to feature vectors.\n",
    "  - **Projection Head**: A small MLP maps features to a space where contrastive loss is applied.\n",
    "  - **Contrastive Loss**: NT-Xent loss (Normalized Temperature-scaled Cross Entropy) encourages similar pairs to be close.\n",
    "\n",
    "- **Training part**:\n",
    "  - No labels needed during contrastive training.\n",
    "  - After training, the encoder is used as a feature extractor.\n",
    "  - A linear classifier can be trained on top of frozen features for downstream tasks (e.g., classification).\n",
    "\n",
    "---\n",
    "\n",
    "The goal of the paper was to show that **good augmentations + contrastive loss + projection head** can produce powerful representations,neural embeddings, even without labeled data. It's a foundational idea in modern self-supervised learning used for instance for transformers!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150453a7",
   "metadata": {},
   "source": [
    "# Preparing our augmentations\n",
    "Wonder what they are, Google them! Important is: we want the output to be invariant to the transformation as much as possible (cat with sunglasses is also a cat, remember?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----- Data Augmentation -----\n",
    "class SimCLRPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.augment = tf.keras.Sequential([\n",
    "            layers.Rescaling(1./255),\n",
    "            layers.RandomCrop(28, 28),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),\n",
    "            layers.RandomZoom(0.2),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.augment(x), self.augment(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a23d6",
   "metadata": {},
   "source": [
    "# Make the encoder and projection head to z (where we do the cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a926e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Encoder and Projection Head -----\n",
    "def create_encoder(projection_dim=32):\n",
    "    base_model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, 3, strides=2, activation='relu'),\n",
    "        layers.Conv2D(64, 3, strides=2, activation='relu'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "    ])\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    features = base_model(inputs)\n",
    "    # Projection head, the z's remember?\n",
    "    outputs = tf.keras.Sequential([\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(projection_dim)\n",
    "    ])(features)\n",
    "    # Normalize to unit vectors so dot product equals cosine similarity (required for contrastive loss)\n",
    "    outputs = tf.math.l2_normalize(outputs, axis=1)\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c58be",
   "metadata": {},
   "source": [
    "# Then the contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is complicated, let's imolement NT-Xent\n",
    "def contrastive_loss(z1, z2, temperature=0.5):\n",
    "    # First: Concatenate both batches of embeddings (positive pairs)\n",
    "    z = tf.concat([z1, z2], axis=0)  # shape: (2N, D), where N is batch size\n",
    "\n",
    "    # Cosine similarity matrix between all embeddings (assumes z is L2-normalized)\n",
    "    sim = tf.matmul(z, z, transpose_b=True)  # shape: (2N, 2N), sim[i][j] = similarity between sample i and j\n",
    "    sim /= temperature  # scale similarities by temperature (sharpening)\n",
    "\n",
    "    # Create some positive/negative pair labels — position i matches with i + N ( same image, different view)\n",
    "    batch_size = tf.shape(z1)[0]\n",
    "    labels = tf.range(batch_size)\n",
    "    labels = tf.concat([labels, labels], axis=0)  # shape: (2N,)\n",
    "\n",
    "    # Remove self-similarities (the diagonal) from similarity matrix, dont need to do similarity with itselt\n",
    "    mask = tf.eye(2 * batch_size)  # identity matrix\n",
    "    sim = sim - 1e9 * mask  # set diagonal to a large negative number so it's ignored in softmax, HACKY! COuld do masking but expensive\n",
    "\n",
    "    # Get positive similarities from the similarity matrix\n",
    "    # Positive pairs are offset by +N and -N in the 2N batch\n",
    "    positives = tf.concat([\n",
    "        tf.linalg.diag_part(sim, k=batch_size),   # sim[i][i+N]\n",
    "        tf.linalg.diag_part(sim, k=-batch_size)   # sim[i+N][i]\n",
    "    ], axis=0)  # shape: (2N,)\n",
    "\n",
    "    # Step 6: Compute the famous NT-Xent loss\n",
    "    numerator = tf.exp(positives)  # exp(similarity of positive pairs)\n",
    "    denominator = tf.reduce_sum(tf.exp(sim), axis=1)  # sum over all other similarities for each sample\n",
    "    loss = -tf.math.log(numerator / denominator)  # -log(positive / all)\n",
    "    \n",
    "    # Step 7: Return average loss over the batch\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fe7a4",
   "metadata": {},
   "source": [
    "# Fetch data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec249101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Data -----\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")#→ (N, 28, 28, 1), a different way of adding a channel\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")#→ (N, 28, 28, 1)\n",
    "\n",
    "batch_size = 512\n",
    "augment = SimCLRPreprocessing()\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    .shuffle(1024)\n",
    "    .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "#Returns: \n",
    "# x1: (batch_size, 28, 28, 1), x2: (batch_size, 28, 28, 1) --> Only augmented images, not the initial image!\n",
    "\n",
    "# ----- Train SimCLR Encoder -----\n",
    "model = create_encoder()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(20):\n",
    "    losses = []\n",
    "    for x1, x2 in train_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            z1 = model(x1, training=True)\n",
    "            z2 = model(x2, training=True)\n",
    "            loss = contrastive_loss(z1, z2)\n",
    "        grads = tape.gradient(loss, model.trainable_weights) #gradients of the loss wrt all trainable weights in the model\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights)) #applies the gradients to update the weights using the Adam optimizer we set (zip matches gradients to weights)\n",
    "        losses.append(loss.numpy()) #store loss so we can track it\n",
    "    print(f\"Epoch {epoch+1}: Loss = {np.mean(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bd44c",
   "metadata": {},
   "source": [
    "# Cool! We made our first embedding. \n",
    "Let's use t-SNE to vizualize the embedding. We haven't trained with any labels, is our model still able to tell the numbers apart, although trained unsupervised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea173879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----- Embedding Visualization -----\n",
    "# Extract features and apply t-SNE\n",
    "features = model(x_test / 255.).numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, init='random', random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='tab10', alpha=0.6)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.title(\"t-SNE of SimCLR embeddings (MNIST)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd802a7",
   "metadata": {},
   "source": [
    "# Linear evaluation\n",
    "We can also for instance now freeze the embedding, and train a linear classifier on top (\"finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Linear Evaluation -----\n",
    "# Freeze encoder, and train a inear classifier based on the embedding!!\n",
    "frozen_encoder = tf.keras.Sequential([\n",
    "    model,\n",
    "    layers.Dense(64, activation='relu', trainable=False)\n",
    "])\n",
    "classifier = tf.keras.Sequential([\n",
    "    frozen_encoder,\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.fit(x_train / 255., y_train, epochs=5, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And for the classifier\n",
    "features = classifier(x_test / 255.).numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, init='random', random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=y_test, cmap='tab10', alpha=0.6)\n",
    "plt.colorbar(scatter, ticks=range(10))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "axol1tl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
